#
# General system settings
#
- name: enable EPEL yum repo
  yum:
    name: epel-release

- name: yum
  yum:
    name:
      - screen
      - tmux
      - htop
      - iotop
      - netcdf

- name: mail relay config
  template:
    src: postfix/main.cf
    dest: /etc/postfix/main.cf
  notify: reload postfix

- name: mail aliases
  template:
    src: postfix/aliases
    dest: /etc/aliases
  register: newalias

- name: newaliases
  shell: /usr/bin/newaliases
  when: newalias.changed

- name: chrony.conf
  template:
    src: chrony.conf
    dest: /etc/chrony.conf
  notify:
    restart chronyd

- name: chrony service
  service:
    name: chronyd
    state: started
    enabled: true


#
# Permission structures to allow selected users to manage data
#
- name: user datag
  user:
    name: datag
    uid: "{{datag_uid}}"

- name: Set UID range for human users
  lineinfile:
    path: /etc/login.defs
    regexp: "^UID_MAX"
    line: UID_MAX 1099 # reduce from default of 60000, which overlaps with our system users

- name: Set GID range for human users
  lineinfile:
    path: /etc/login.defs
    regexp: "^GID_MAX"
    line: GID_MAX 1099 # reduce from default of 60000, which overlaps with our system users

- name: user accounts
  user:
    name: "{{item.key}}"
    uid: "{{item.value}}"
    groups: datag
    append: yes
  loop: "{{datag_users | dict2items}}"


#
# Directories
#
- name: parent of production and user data dirs
  file:
    path: "{{dataset_parent_dir}}"
    state: directory
    owner: root
    group: root
    mode: u=rwx,g=rx,o=rx

- name: data dir
  file:
    path: "{{data_dir}}"
    state: directory
    owner: datag
    group: datag
    mode: u=rwx,g=rwxs,o=rx

- name: datalib dir
  file:
    path: "{{dldir}}"
    state: directory

- name: datalib etc dir
  file:
    path: "{{dldir}}/etc"
    state: directory

- name: DL user dirs parent
  file:
    path: "{{dl_homes_dir}}"
    state: directory
    owner: root
    group: root
    mode: u=rwx,g=rx,o=rx

- name: DL user dirs
  file:
    path: "{{dl_homes_dir}}/{{item}}"
    state: directory
    owner: "{{item}}"
    group: "{{item}}"
    mode: u=rwx,g=rx,o=rx
  with_items: "{{datag_users}}"

- name: DL user homes
  file:
    path: "{{dl_homes_dir}}/{{item}}/DataCatalog"
    state: directory
    owner: "{{item}}"
    group: "{{item}}"
    mode: u=rwx,g=rx,o=rx
  with_items: "{{datag_users}}"

#
# Docker and docker-compose
#
- set_fact:
    docker_python: "/usr/bin/python3"

- name: docker dir
  file:
    state: directory
    path: "{{docker_dir}}"
    mode: "u=rwx,g=x,o=x"

- name: docker dir symlink
  file:
    state: link
    src: "{{docker_dir}}"
    dest: /var/lib/docker
  when: docker_dir != "/var/lib/docker"

- name: install docker, docker-compose
  yum:
    name:
      - docker
      - docker-compose

- name: docker config file
  copy:
    dest: /etc/docker/daemon.json
    content: >
      {
        "registry-mirrors": ["{{docker_registry_mirror}}"],
        "insecure-registries": ["{{docker_registry_mirror}}"],
        "max-concurrent-downloads": {{docker_max_concurrent_downloads | int}}
      }

  notify: restart docker

- name: start and enable docker
  service:
    name: docker
    state: started
    enabled: true

- name: docker login
  docker_login:
    username: iridlserver
    password: "{{docker_hub_access_token}}"
  vars:
    ansible_python_interpreter: "{{docker_python}}"
  when: docker_hub_access_token is defined
  # Intentionally not logging out. The (read-only) access token is
  # stored unencrypted on the server, and we're ok with that.

- name: maproom-dev image
  docker_image:
    name: iridl/maproom-dev:{{maproom_dev_version}}
    source: pull
  vars:
    ansible_python_interpreter: "{{docker_python}}"


#
# squid
#
- name: squid log rotation cron job
  cron:
    name: squid log rotation
    user: root
    minute: "0"
    hour: "0"
    job: docker exec {{compose_project_name}}_squid_1 squid -k rotate

- set_fact:
    squid_config_dir: "{{dldir}}/squid"
    squid_shutdown_lifetime: 30

- name: squid group
  group:
    name: squid
    gid: 23 # the UID/GID used by the CentOS squid packages

- name: squid user
  user:
    name: squid
    uid: 23
    shell: /sbin/nologin
    create_home: no

- name: squid config directory
  file:
    path: "{{squid_config_dir}}"
    state: directory

- name: squid.conf
  template:
    src: squid.conf
    dest: "{{squid_config_dir}}/squid.conf"
  vars:
    dl_hostport_regex: "{{ dl_hostport | replace('.', '\\.') }}"
  notify: reconfigure squid


#
# git and access to git repos
#
- name: install git
  yum:
    name: git

- name: /root/.ssh
  # Necessary when using sudo, as in vagrant. Works around a bug in
  # the known_hosts module, which ought to create the directory if it
  # doesn't exist.
  file:
    path: /root/.ssh
    state: directory
    mode: 0700

- name: add git host key to ssh_known_hosts
  known_hosts:
    name: "{{git_host}}"
    # This is a public key, not a secret, so no need to encrypt it.
    key: "{{git_host_key}}"

- name: ssh access key directory
  # let's keep all data library files together
  file:
    path: "{{dldir}}/ssh"
    state: directory
    mode: 0700

- set_fact:
    git_key_file: "{{dldir}}/ssh/id_rsa-git"

- name: git ssh key
  copy:
    dest: "{{git_key_file}}"
    content: "{{git_key}}"
    mode: 0600

- name: ssh command for git
  copy:
    dest: "/usr/local/bin/ssh-git"
    mode: 0744
    content: |
      #!/bin/sh
      ssh -i {{git_key_file}} -o IdentitiesOnly=yes "$@"

#
# Scripts for managing data
#
- set_fact:
    update_script: /usr/local/bin/update_datalib

- name: update script
  template:
    dest: "{{update_script}}"
    mode: 0744
    src: "update_datalib"

- name: datag sudoers config
  copy:
    content: |
      %datag ALL=(root) NOPASSWD: {{update_script}}
    dest: /etc/sudoers.d/datag_update
    mode: 0400

- name: run update script
  command: "bash -x {{update_script}} --notty"
  when: run_update_script | bool

- name: sql import dir
  file:
    path: "{{sql_dir}}"
    state: directory
    owner: datag
    group: datag
    mode: u=rwx,g=rwxs,o=rx

- name: sql import script
  copy:
    content: |
      #!/usr/bin/bash -l

      docker exec -it {{compose_project_name}}_postgres_1 /usr/local/bin/entrypoint.sh execsql iridb
    dest: /usr/local/bin/sql_exec
    mode: 0744

- name: sql command script
  copy:
    content: |
      #!/usr/bin/bash -l

      if [[ -t 0 ]]; then  # input is coming from an interactive terminal
        tty_arg=-t
      fi
      docker exec -i $tty_arg {{compose_project_name}}_postgres_1 psql -U postgres iridb
    dest: /usr/local/bin/sql_interactive
    mode: 0744

- name: sql sudoers config
  copy:
    content: |
      %datag ALL=(root) NOPASSWD: /usr/local/bin/sql_exec
      %datag ALL=(root) NOPASSWD: /usr/local/bin/sql_interactive
    dest: /etc/sudoers.d/datag_sql
    mode: 0400


#
# ingrid
#
- name: ingrid config directory
  file:
    path: "{{ingrid_config_dir}}"
    state: directory

- name: install ingrid localdefs.tex
  template:
    src: ingrid-localdefs.tex
    dest: "{{ingrid_config_dir}}/localdefs.tex"
  notify: restart ingrid

- name: ingrid cleantmp cron job
  cron:
    name: cleantmp
    user: root
    minute: "2,7,12,17,22,27,32,37,42,47,52,57"
    job: docker exec -u {{ingrid_uid}} {{compose_project_name}}_ingrid_1 /opt/ingrid/bin/cleantmp.pl {{ admin_emails | join(" ")}}

- name: ingrid scanproc cron job
  cron:
    name: scanproc
    user: root
    minute: "*/15"
    job: "docker exec {{compose_project_name}}_ingrid_1 /opt/ingrid/bin/scanproc.pl /dev/null 2>&1"



#
# put it all together with docker-compose
#
- name: docker-compose project dir
  file:
    path: "{{compose_project_dir}}"
    state: directory
    
- name: docker-compose.yaml
  template:
    src: docker-compose.yaml
    dest: "{{compose_project_dir}}/docker-compose.yaml"

- name: update docker-compose state
  docker_compose:
    project_src: "{{compose_project_dir}}"
    pull: yes
    remove_orphans: yes
    debug: yes # to get info on what changed (module doesn't support --diff)
    timeout: "{{squid_shutdown_lifetime + 10}}" # See comment in "restart squid" handler
  vars:
    ansible_python_interpreter: "{{docker_python}}"
  register: docker_compose_result

- debug:
    msg: "{{docker_compose_result.actions}}"

# TODO need smarter criteria for pruning old images; this prunes the
# latest maproom-dev, which then gets re-downloaded on every maproom
# build.
# - name: remove unused docker images
#   docker_prune:
#     images: yes
#     images_filters:
#       dangling: false
#   vars:
#     ansible_python_interpreter: "{{docker_python}}"


#
# Flush handlers, then run tests.
#
# TODO: Ansible gets service restarts wrong. If a playbook crashes
# after a notify but before running the handler, then the handler
# doesn't get triggered on the next run. Stop using handlers and do it
# right: touch a file when the service needs to be restarted, add a
# task that checks for that file and, if it exists, restarts the
# service and then removes the file.

- meta: flush_handlers

- name: service tests
  uri:
    url: http://{{dl_hostport}}{{item}}
    headers:
      Cache-Control: no-cache # force squid to talk to the backend
  retries: 5
  delay: 1
  register: uri_result
  until: uri_result is success
  delegate_to: localhost
  become: false
  with_items:
    - /
    - /expert  # ingrid
    - /SOURCES/ # verify data volume is mounted
    - /home/{{(datag_users.keys() | list)[0]}}/
    - /maproom/
  changed_when: no
  check_mode: no # run even in check mode

- name: test outbound proxy
  command: 'docker exec {{compose_project_name}}_ingrid_1 bash -c "curl --silent --header \"Cache-Control: no-cache\" -o /dev/null -w %{http_code} google.com"'
  register: outbound_test
  check_mode: no
  changed_when: no
  failed_when: outbound_test.stdout != "301"

- name: verify that ingrid can talk to db
  uri:
    url: http://{{dl_hostport}}/IRIDB/(select%20count(*)%20from%20nosuchtable)/openquery/
    headers:
      Cache-Control: no-cache
    return_content: yes
  retries: 5
  delay: 5
  register: result
  until: result is success
  delegate_to: localhost
  become: false
  # If we can't talk to the db then we get a different error message.
  # Once we have a test db fixture, change the test to look for an actual table.
  failed_when: "'ERROR:  relation \"nosuchtable\" does not exist' not in result.content"
  check_mode: no

- name: verify that chronyd has synchronized with a time server
  shell: chronyc sources | grep '^\^\*'
  register: chronyc_result
  retries: 6
  until: chronyc_result is success
  delay: 10
  changed_when: no
  check_mode: no # run even in check mode
